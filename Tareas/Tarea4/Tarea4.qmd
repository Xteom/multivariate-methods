---
title: 'Métodos Multivariados: Tarea 4'
author: "Aldo Carmona, Diego Arellano, Mateo De La Roche, Victor Contreras"
published_date: "27/02/2024"
format: pdf
editor: visual
output:
  pdf_document: 
    latex_engine: xelatex
  html_document: default
editor_options:
  chunk_output_type: console
---

## Ejercicio 1

1.  Si dos variables $X$ y $Y$ tienen covarianza $S = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$, entonces mostrar que si $c \neq 0$ entonces la primera componente principal está dada por:

$$
\sqrt{\frac{c^2}{c^2 + (V_1 - d)^2}} X + \frac{c}{|c|} \sqrt{\frac{(V_i - d)^2}{c^2 + (V_1 - d)^2}} Y,
$$

donde $V_1$ es la varianza explicada por la primera componente principal. ¿Cuál es el valor de $V_1$?

Consideremos la matriz de covarianza $S$ para dos variables $X$ y $Y$:

$$
S = \begin{pmatrix}
a & b \\
c & d
\end{pmatrix}.
$$

Para encontrar los valores propios $\lambda$, resolvemos la ecuación característica:

$$
\det(S - \lambda I) = 0.
$$

Desarrollando el determinante, obtenemos:

$$
\begin{vmatrix}
a - \lambda & b \\
c & d - \lambda
\end{vmatrix}
= (a - \lambda)(d - \lambda) - bc = \lambda^2 - (a + d)\lambda + (ad - bc) = 0.
$$

Las soluciones de esta ecuación cuadrática son los valores propios $\lambda_1$ y $\lambda_2$, que se calculan como:

$$
\lambda_{1,2} = \frac{1}{2} \left(a + d \pm \sqrt{(a - d)^2 + 4bc}\right).
$$

Dado que $V_1$ es el mayor valor propio, se corresponde con:

$$
V_1 = \frac{1}{2} \left(a + d + \sqrt{(a - d)^2 + 4bc}\right).
$$

Ahora, para encontrar el vector propio asociado a $V_1$, resolvemos $(S - V_1 I) \vec{v} = 0$. Suponiendo que el vector propio es $\vec{v} = (x, y)^T$, tenemos:

$$
\begin{pmatrix}
a - V_1 & b \\
c & d - V_1
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix}
= \begin{pmatrix}
0 \\
0
\end{pmatrix}.
$$

Dado $c \neq 0$, podemos expresar $y$ en términos de $x$:

$$
cx + (d - V_1)y = 0 \quad \Rightarrow \quad y = -\frac{c}{d - V_1}x.
$$

Para que el vector propio tenga longitud unitaria, $x^2 + y^2 = 1$, sustituimos $y$ en términos de $x$:

$$
x^2 + \left(-\frac{c}{d - V_1}x\right)^2 = 1.
$$

Resolviendo para $x$, obtenemos:

$$
x = \sqrt{\frac{1}{1 + \left(\frac{c}{d - V_1}\right)^2}} = \sqrt{\frac{(d - V_1)^2}{c^2 + (d - V_1)^2}}.
$$

Y para $y$, usando la relación $y = -\frac{c}{d - V_1}x$, obtenemos:

$$
y = \frac{c}{\sqrt{c^2 + (V_1 - d)^2}}.
$$

Así, la primera componente principal está dada por:

$$
\sqrt{\frac{c^2}{c^2 + (V_1 - d)^2}} X + \frac{c}{|c|} \sqrt{\frac{(V_1 - d)^2}{c^2 + (V_1 - d)^2}} Y,
$$

donde $V_1$ es la varianza explicada por la primera componente principal.

## Ejercicio 2

Considerar los datos en el archivo `T8-5.DAT` correspondientes a un tramo censal. Suponer que las observaciones en la variable $X_5 =$ valor de la mediana de hogares fue registrada en unidades de diez miles más que de cientos de miles de dólares (es decir, multipliquen todos los datos listados en la quinta columna por 10).

```{r}
data_ej2 <- read.table("T8-5.DAT", header = TRUE)
data_ej2_adj <- data_ej2
data_ej2_adj[, 5] <- data_ej2_adj[, 5] * 10
```

### a. Comparación de estimaciones.

Comparamos las estimaciones con los datos en diez miles y cientos de miles (son dos matrices de covarianzas) para las componentes principales en cada caso.

```{r}
# covarianzas 
cov_data_ej2 <- cov(data_ej2)
cov_data_ej2_adj <- cov(data_ej2_adj)

# pca
# scale = FALSE porque nos interesa ver como cambian al ser unidades diferentes
pc_dataej2 <- prcomp(data_ej2, scale = FALSE)
pc_dataej2_adj <- prcomp(data_ej2_adj, scale = FALSE)
```

Matriz de covarianza (datos sin ajustar)

```{r, echo=FALSE}
cov_data_ej2
```


Matriz de covarianza (datos ajustados)
```{r, echo=FALSE}
cov_data_ej2_adj
```

Componentes principales (datos sin ajustar)
```{r}
summary(pc_dataej2)

```

Componentes principales (datos ajustados)
```{r}
summary(pc_dataej2_adj)
```


### b. Interpretación de componentes
Con los datos no ajustados la primera componente principal nos indica el negativo de $X_4$ y la segunda nos indica el negativo de $X_3$
```{r}
pc_dataej2$rotation
```

Con los datos ajustados, la primera componente principal nos indica $X_4$ y la segunda compente princial nos indica $X_3$ y ahora también tiene también una fuerte componente de $X_5$

```{r}
pc_dataej2_adj$rotation
```

### c. Efectos en el cambio de escala.

Podemos ver como los efectos de la variable $X_5$ en verdad erán más importantes, sin embargo como no estaba bien reescalada, el primer análisis de ocmponentes principales, no lo tomó en cuenta.

### Extra (utilizando Scale = True)

```{r}
pc_dataej2_extra <- prcomp(data_ej2, scale = TRUE)
pc_dataej2_extra$rotation

```

Al estandarizar las variables vemos como todas las cargas de las variables en las componentes principales cambian por completo.

## Ejercicio 3

Considerar los datos sobre toros en el conjunto de datos T1 – 10.DAT sobre toros. Estos datos contienen las características medidas de 76 toros jóvenes (menores a dos años) vendidos en una subasta. Los datos que se incluyen corresponden a las siguientes variables:

-   `Raza`: 1= Angus, 5= Hereford, 8= Simmental
-   `PVenta`: precio de venta
-   `YrHgt`: medición al hombro al año (pulgadas)
-   `FtFreBody`: Cuerpo libre de grasa (libras)
-   `PrctFFB`: Porcentaje del cuerpo libre de grasa
-   `Frame`: Cornamenta. Escala de 1 (pequeña) a 8 (grande)
-   `BkFat`: Grasa trasera (en pulgadas)
-   `SaleHt`: medición al hombro en el momento de venta (pulgadas)
-   `SaleWt`: peso de venta (libras)

Utilizando las 7 últimas variables dadas, hacer un análisis de componentes principales usando la matriz de covarianzas de los datos y la matriz de correlación. El análisis debe incluir lo siguiente:

### a. Número de componentes

Para determinar el número de componentes apropiadas utilizamos un Scree plot
```{r}
dataej3 <- read.table("T1-10.DAT", header=TRUE)
colnames(dataej3) <- c("Raza", "PVenta", "YrHgt", "FtFreBody", "PrctFFB", 
                       "Frame", "BkFat", "SaleHt", "SaleWt")

#quitamos datos categóricos 
dataej3_pca <- dataej3[, c("PVenta", "YrHgt", "FtFreBody", "PrctFFB", 
                           "Frame", "BkFat", "SaleHt", "SaleWt")]


pca_dataej3 <- prcomp(dataej3_pca, scale = TRUE)
summary(pca_dataej3)
#plot(pca_dataej3, type = "l") 

library(factoextra)
fviz_screeplot(pca_dataej3,ncp=11)

```

Parece ser que con tres componentes tenemos una buena explicabilidad de la varianza.

### b. Interpretación de componentes

Interpretación de las componentes principales.
```{r}
pca_dataej3$rotation[, c("PC1", "PC2", "PC3")]
```
La primera componente principal nos indica que tan corta es la distancia al hombro en el momento de la venta, que tan chica es la cornamenta y que tan pequeña fue la medición al hombro luego de un año. Es decir que mientras mayor sea la primera componente principal de un toro, menor serán esas tres características. 

La segunda componente principal, nos indica el el precio de venta del toro, la cantidad de grasa trasera y el negativo el porcentaje del cuerpo libre de grasa. Mientras mayor sea esta componente, más caro se vendió el toro, mayor grasa trasera tenía y menos porcentaje del cuerpo libre de grasa tenía. 

La tercera componente principal nos indica el peso de venta y el cuerpo libre de grasa. 

### c. Índice de tamaño de cuerpo

¿Será posible desarrollar un índice 'Tamaño de cuerpo' o 'configuración de cuerpo' basado en las 7 variables consideradas?

El negativo de la primera componente principal pararece servir como ese indice, ya que las cargas de las variables del tamaño del cuerpo del toro son las más altas.

### d. Gráfica de componentes

Hacer una gráfica de las dos primeras componentes. ¿Hay outliers? Si los hay, hacer una sustitución de la matriz de covarianzas con una matriz de covarianzas estimada de manera robusta.

```{r}
fviz_pca_biplot(pca_dataej3,repel = T,geom.ind = "point")

```

### e. Normalidad de datos originales

Evalúen si los datos originales son normales. Si no lo son, buscar las transformaciones que los acerquen a normalidad. Repetir el análisis con los datos transformados y probar la significancia de la varianza de las componentes principales con el resultado de Anderson.

```{r}
apply(dataej3_pca, 2, shapiro.test)
```

podemos ver que las columnas Pventa, FtFreBody (a pesar de tener una W alta, tenemos un p value bajo lo que indica que es poco probale que sean normales), Frame y BkFat no siguen distribución normal. 

```{r, echo=FALSE}
# Asegúrate de tener tu dataframe dataej3_pca listo
par(mfrow=c(2,2)) # layout del plot en 2 filas y 2 columnas

# Histograma para PVenta
hist(dataej3_pca$PVenta, main="Histograma de PVenta", xlab="PVenta", col="skyblue", border="black")

# Histograma para FtFreBody
hist(dataej3_pca$FtFreBody, main="Histograma de FtFreBody", xlab="FtFreBody", col="skyblue", border="black")

# Histograma para Frame
hist(dataej3_pca$Frame, main="Histograma de Frame", xlab="Frame", col="skyblue", border="black")

# Histograma para BkFat
hist(dataej3_pca$BkFat, main="Histograma de BkFat", xlab="BkFat", col="skyblue", border="black")

# Restablecer las configuraciones gráficas por defecto
par(mfrow=c(1,1))
```

```{r}
dataej3_pca$PVenta_log <- log(dataej3_pca$PVenta)
dataej3_pca$FtFreBody_log <- log(dataej3_pca$FtFreBody)

dataej3_pca$BkFat_sqrt <- sqrt(dataej3_pca$BkFat)
dataej3_pca$Frame_sqrt <- sqrt(dataej3_pca$Frame) # 

apply(dataej3_pca[, c("PVenta_log", "FtFreBody_log", "BkFat_sqrt", "Frame_sqrt")]
      , 2, shapiro.test)
```


```{r, echo=FALSE}
par(mfrow=c(2,2)) #  layout del plot en 2 filas y 2 columnas

# Histograma para PVenta
hist(dataej3_pca$PVenta_log, main="Histograma de PVenta_log", xlab="PVenta_log", col="skyblue", border="black")

# Histograma para FtFreBody
hist(dataej3_pca$FtFreBody_log, main="Histograma de FtFreBody_log", xlab="FtFreBody_log", col="skyblue", border="black")

# Histograma para Frame
hist(dataej3_pca$Frame_sqrt, main="Histograma de Frame_sqrt", xlab="Frame_sqrt", col="skyblue", border="black")

# Histograma para BkFat
hist(dataej3_pca$BkFat_sqrt, main="Histograma de BkFat_sqrt", xlab="BkFat_sqrt", col="skyblue", border="black")

# Restablecer las configuraciones gráficas por defecto
par(mfrow=c(1,1))


# transformación boxcox pa después
#library(MASS)
#dataej3_pca$PVenta_bc <- boxcox(dataej3_pca$PVenta, lambda = seq(-2, 2, by = 0.1))

```

 
## Ejercicio 4

Consideren la matriz de correlaciones siguiente. Los datos originales corresponden a las mediciones de 8 variables de química sanguínea de 72 pacientes en un estudio clínico. (Jolliffe, 2002). La matriz de correlaciones de las variables rblood, plate, wblood, neut, lymph, bilir, sodium y potass, en ese orden, es la siguiente:

```{r}
ej4_corr <- c(
  1.000, 0.290, -0.202, -0.055, -0.105, -0.252, -0.229,  0.058,
  0.290, 1.000,  0.415,  0.285, -0.376, -0.349, -0.164, -0.129,
 -0.202, 0.415,  1.000,  0.419, -0.521, -0.441, -0.145, -0.076,
 -0.055, 0.285,  0.419,  1.000, -0.877, -0.076,  0.023, -0.131,
 -0.105, -0.376, -0.521, -0.877,  1.000,  0.206,  0.034,  0.151,
 -0.252, -0.349, -0.441, -0.076,  0.206,  1.000,  0.192,  0.077,
 -0.229, -0.164, -0.145,  0.023,  0.034,  0.192,  1.000,  0.423,
  0.058, -0.129, -0.076, -0.131,  0.151,  0.077,  0.423,  1.000
)
```

y las desviaciones estándar, que tienen considerables diferencias, son:

rblood plate wblood neut lymph bilir sodium potass

```{r}
ej4_desv <- c(0.371, 41.253, 1.935, 0.077, 0.071, 4.037, 2.732, 0.297)
```

```{r}
# reconstruimos la matriz de correlaciones
ej4_corr_matrix <- matrix(ej4_corr, nrow = 8, ncol = 8)
# Calculamos la matriz de covarianzas 
ej4_cov_matrix <- ej4_corr_matrix * (ej4_desv %*% t(ej4_desv))
```

### a. Componentes Pricipales
```{r}
# PCA usando la matriz de correlaciones
ej4_pca_cor <- prcomp(ej4_corr_matrix, scale = TRUE)
summary(ej4_pca_cor)

# PCA usando la matriz de covarianzas
ej4_pca_cov <- prcomp(ej4_cov_matrix, scale = FALSE)
summary(ej4_pca_cov)
```

Aplicar componentes principales a la matriz de covarianzas y a la matriz de correlaciones. Explicar las diferencias.

Para el PCA basado en la matriz de correlaciones, vemos que:

PC1 explica el 54.31% de la varianza.
PC2 explica el 20.32% de la varianza.
Sumando hasta la tercera componente principal, explican el 87.02% de la varianza.
Para el PCA basado en la matriz de covarianzas, el primer componente principal explica prácticamente toda la varianza (99.99%)

### b. Interpretación para análisis

Basado en la observación anterior, sobre qué debería hacerse el análisis?

Podemos ver que es mejor hacer el pca con la matriz de correlaciones ya que la varianza no se encuentra tan cargada en la primer componente principal como pasa con la matriz de covarianzas. Al usar la matriz de correlaciones permitimos que cada variable aporte de forma significativa al análisis. No dejamos que una sola variable de gran varianza a escala domine todo el análisis.


## Ejercicio 5

Encontrar las componentes principales de la siguiente matriz de correlación calculada de las mediciones de 7 características físicas en 3,000 convictos criminales: Las variables son: 1. largo de la cabeza, 2. ancho de la cabeza, 3. ancho de la cara, 4. longitud del dedo pulgar izquierdo, 5. longitud del antebrazo izquierdo, 6. longitud del pie izquierdo, 7. Altura.

$$
\begin{bmatrix}
1     &       &      &     &     &     &     \\
0.402 & 1     &      &     &     &     &     \\
0.396 & 0.618 & 1     &      &     &     &     \\
0.301 & 0.150 & 0.321 & 1     &      &      &      \\
0.305 & 0.135 & 0.289 & 0.846 & 1     &      &      \\
0.339 & 0.206 & 0.365 & 0.759 & 0.797 & 1     &      \\
0.340 & 0.183 & 0.345 & 0.661 & 0.800 & 0.736 & 1     \\
\end{bmatrix}
$$

```{r, echo=FALSE}
ej5_corr_matrix <- matrix(c(   1,     0.402, 0.396, 0.301, 0.305, 0.339, 0.340,   0.402, 1,     0.618, 0.150, 0.135, 0.206, 0.183,   0.396, 0.618, 1,     0.321, 0.289, 0.365, 0.345,   0.301, 0.150, 0.321, 1,     0.846, 0.759, 0.661,   0.305, 0.135, 0.289, 0.846, 1,     0.797, 0.800,   0.339, 0.206, 0.365, 0.759, 0.797, 1,     0.736,   0.340, 0.183, 0.345, 0.661, 0.800, 0.736, 1 ), nrow = 7, byrow = TRUE)
```

```{r}
var_names <-  c("LargoCabeza", "AnchoCabeza", "AnchoCara", "LongPulgarIzq", 
                "LongAntebrazoIzq", "LongPieIzq", "Altura")

rownames(ej5_corr_matrix) <- var_names
colnames(ej5_corr_matrix) <- var_names
# Realizar PCA
ej5_pca <- prcomp(ej5_corr_matrix)

summary(ej5_pca)

# cargas
ej5_pca$rotation
```

