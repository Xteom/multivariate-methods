---
title: 'Lab2: Normalidad Multivariada'
author: "Jorge de la Vega"
date: "1/2/2024"
format: 
  html:
    html-math-method: katex
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rgl)
knitr::knit_hooks$set(webgl = hook_webgl)
```

## Normal univariada

```{r}
curve(dnorm(x, mean = 0, sd = 1), from = -4, to = 4)  # densidad
# Regla empírica: 
abline(v = qnorm(c(1-0.68,0.68,1-0.95,0.95,1-0.99,0.99)), col = c(rep("blue",2),rep("green",2),rep("red",2)))
abline(h=0)

curve(pnorm(x, mean = 0, sd = 1), from = -4, to = 4)  # distribución
x <- rnorm(1000)
hist(x, breaks = 50, prob = T)  # muestra 
curve(dnorm(x, mean = 0, sd = 1), from = -4, to = 4, add = T, col = "red", lwd = 3)  # densidad
rug(x)
```


## Normales bivariadas

Para el caso multivariado hay varias opciones que se pueden considerar.

```{r, webgl=T}
library(mvtnorm) # distribuciones normal y t multivariadas
library(MASS)    # función mvrnorm para generar muestras de normales multivariadas
library(mnormt)  # equivalentes multivariados de normal y t, incluye truncamiento.
x <- y <- seq(-5, 5, 0.1)
Mu <- c(0,1)      # vector de medias
Sigma <- matrix(c(1, 0.75, 0.75, 1), nrow = 2) # matriz de covarianzas
f <- function(x,y) dmnorm(cbind(x,y), mean = Mu, varcov = Sigma) # de paquete mnormt
f(1,2)  # evalua la densidad a un real
# Gráficas
z <- outer(x,y,f) # evalua f en cada x y y de un grid
persp(x,y,z, col = "green",phi = 30, theta = -20, shade = 0.1, r = 10, d = 5)
rgl::persp3d(x,y,z, col = "green")  # interactivo
```


```{r}
contour(x, y, z, col = "blue", nlevels = 10, 
        main = "Curvas de nivel Normal bivariada en el dominio",
        xlim = c(-3,3), ylim = c(-2,3.5))  # curvas de nivel

X <- rmnorm(n = 1000, mean = Mu, varcov = Sigma)  # muestra normal multivariada de mnormt
# también se puede usar la función rmvnorm de MASS
Y <- mvrnorm(n = 1000, mu = Mu, Sigma = Sigma)  # muestra normal multivariada
H <- kde2d(Y[,1], Y[,2], n = 500) # objeto a grafica(en MASS)
contour(H)  # curvas de concentración
image(H)    # versión como mapa de calor, se pueden cambiar colores.
```


### Geometría de la normal bivariada

```{r}
Sinv <- solve(matrix(c(3,1,1,3),ncol=2)) #inversa de la matriz de covarianzas
mu <- c(3,3) #vector media
DE <- eigen(Sinv) # descomposición espectral de Sinv
# función del lugar geométrico y version para vectorizar
elipse <- function(x, y, mu, Sigma){as.numeric((c(x,y) - mu) %*% Sigma %*% (c(x,y) - mu))}
elipse2 <- function(x, y)apply(cbind(x, y), 1, function(x)elipse(x[1], x[2], mu = mu, Sigma = Sinv))

x <- seq(-1, 10, 0.01)

par(pty = "s") # Para hacer una gráfica cuadrada (no se distorsionen las escalas)
contour(x, x, outer(x, x, elipse2), levels = c(0.1, 0.2, 0.4, 1, 2, 3), xlim = c(0,7), ylim = c(0,7),
        main = "Rotación a componentes principales", xlab = "x1", ylab = "x2")

points(mu[1], mu[2], pch = 16, cex = 0.9) # centro

# Ejes en la dirección de las componentes principales (dibuja las flechas): 
# los vectes propios de la matriz de covarianzas son las direcciones de máxima varianza 
s <- 2 # factor de escala
arrows(x0 = mu[1], y0 = mu[2],
x1 = mu[1] + s*DE$vectors[1,1], y1 = mu[2] + s*DE$vectors[2,1], length = 0.1, col = "red")
arrows(x0 = mu[1], y0 = mu[2],
x1 = mu[1] + s*DE$vectors[1,2], y1 = mu[2] + s*DE$vectors[2,2], length = 0.1, col = "red")
abline(h = 3, v = 3, lty = 3)
```


## Evaluación de Normalidad

-   En muchos contextos, es conveniente evaluar la hipótesis de que los datos siguen una distribución normal multivariada, ya que varios métodos multivariados suponen normalidad para poder realizar inferencia.

-   Consideremos algunos ejemplos prácticos en este laboratorio.

### Ejercicio 1.

Los datos siguientes (Tabla 1.5) son 42 medidas sobre variables sobre contaminación del aire registradas al mediodía en los Angeles sobre diferentes días. Las variables que se incluyen son:

-   $x_1$ Viento
-   $x_2$ Radiación solar
-   $x_3$ $CO$
-   $x_4$ $NO$
-   $x_5$ $NO_2$
-   $x_6$ $O_3$
-   $x_7$ $HC$

Datos similares se pueden encontrar para la Ciudad de México, más completo, en la siguiente [liga](http://www.aire.cdmx.gob.mx/default.php?opc=%27aKBh%27), aunque requieren más trabajo previo.

1.  Construir un qq-plot para las mediciones de radiación solar y probar normalidad basado en el coeficiente de correlación $r_Q$ al nivel $\alpha = 0.05$ (prueba de Shapiro-Wilks).

```{r}
X <- read.delim(file = "https://raw.githubusercontent.com/jvega68/EA3/master/datos/J&W/T1-5.DAT", sep = "",
                header = F, col.names = c("viento","rad",paste0("x",3:7)))
```

Para la variable radiación

```{r}
# qqplot
qqnorm(scale(X$rad))  # estandarizamos los datos para que la comparación sea adecuada. 
qqline(scale(X$rad))

# histograma: 
hist(X$rad)
```

El coeficiente de correlación $r_Q$ entre los valores observados y los cuantiles teóricos se puede obtener directamente de la función `qqnorm`

```{r}
a <- qqnorm(X$rad, plot.it = F)
(r_Q <- cor(a$x,a$y)) # no se requiere estandarizar 
# Podemos obtener la prueba de Shapiro-Wilk directamente:
shapiro.test(X$rad) #H0: los datos son normales vs Ha: los datos no son normales
```

Se concluye que los datos **no** son normales.

Calculamos las correlaciones para cada variable: 

```{r}
correlaciones <- numeric()
for(i in 1:5){
  z <- qqnorm(X[,i],plot.it = F) #Sólo toma las estadísticas
  correlaciones[i] <- cor(z$x,z$y)
}
names(correlaciones) <- names(X)[1:5]
correlaciones
```

Aunque las correlaciones se ven altas, hay que comparar contra un valor crítico. Esta prueba basada en el qq-plot y el coeficiente de correlación, es la prueba de Filliben (1975). La hipótesis nula es que los datos provienen de una distribución normal, comparando los cuantiles teóricos con los muestrales a través de su correlación.

```{r}
library(ppcc) # Probability Plot Correlation Coefficient test
prueba <- list(NULL)
for(i in 1:7) prueba[[i]] <- ppccTest(X[,i],"qnorm") #7 para que sean todos, antes eran 5 
prueba
```




Ahora se evalúa la regla empírica en cada variable. La siguiente función estandariza las variables y calcula las proporciones de observaciones que están entre más menos 1, 2 y 3 desviaciones estándares


```{r}
regla.empirica <- function(x, grafica = T, titulo){
    # función para evaluar la regla empírica y hacer una grafica diagnóstico
    z <- (x - mean(x))/sd(x)  #normaliza
    n <- length(z)
    #calcula la proporción de oservaciones a 1,2,3 desviaciones estándar
    props <- c(sd1=length(which(z > -1 & z < 1))/n,
               sd2=length(which(z > -2 & z < 2))/n,
               sd3=length(which(z > -3 & z < 3))/n)
    if(grafica==T){
    plot(z,xlab="indice",ylab="valores estandarizados",ylim=c(-4,4),
         main=titulo)
    abline(h=0)
    abline(h=c(-1,1), col = "green")
    abline(h=c(-2,2), col = "yellow")
    abline(h=c(-3,3), col = "red")
    text(n,-c(-4,-3.6,-3.2),paste0(round(100*props,3),"%"),cex=.75)
    }
    return(props=props)
}

par(mfrow=c(2,4))
ReglaEmpirica <- list(NULL)
for(i in 1:5) ReglaEmpirica[[i]] <- regla.empirica(X[,i], titulo=names(X)[i])
```


2.  Examinar el par de variables $x_5$ y $x_6$ para normalidad bivariada:

-   Calcular las distancias de Mahalanobis para todos los puntos
-   Determinar la proporción de observaciones que caen aproximadamente en el contorno de probabilidad del 50% de la normal bivariada.
-   Construir una gráfica $\chi^2$ con las distancias ordenadas.

```{r}
plot(X$x5, X$x6, pch = 16, cex = 2) # no parecen seguir una distribución normal bivariada. 
Z <- X[,5:6] # extraemos las variables
d2 <- mahalanobis(Z,center = colMeans(Z), cov = cov(Z))

# Aquí podemos usar el hecho de que las distancias de Mahalanobis, se deben distribuir como chi^2(p)
qqplot(d2,rchisq(length(d2),2))
abline(a = 0, b = 1)
# También podemos calcular la proporción de observaciones que caen en el elipsoide de tamaño 50%
sum(d2 <= qchisq(0.5, 2))/42  # se esperaría que hubiera la mitad de las observaciones, hay ~ 62%
```

También podemos añadir las elipses de una normal bivariada en el plano para checar visualmente en este caso:

```{r}
library(ellipse)
plot(Z)
lines(ellipse(cov(Z), centre = colMeans(Z),level = 0.1))
lines(ellipse(cov(Z), centre = colMeans(Z),level = 0.5))
lines(ellipse(cov(Z), centre = colMeans(Z),level = 0.95))
```

### Ejercicio 2.

Los datos siguientes (Tabla 4.6) consisten en 130 observaciones generadas por scores en una prueba psicológica administrada a adolescentes peruanos (edades 15, 16 y 17). Se tienen las siguientes variables:

-   sexo (1 = hombre, 2 = mujer)
-   socio (estaus socioeconómico 1 = bajo, 2 = medio)
-   5 scores de subescalas etiquetadas independencia (indep), soporte (supp), benevolencia (benev), conformidad (conform) y liderazgo (leader).

1.  Examinar cada variable si puede ser normal. Para las que no, ver si se pueden transformar a normalidad aproximada.
2.  Revisar normalidad multivariada

```{r}
X <- read.delim(file = "https://raw.githubusercontent.com/jvega68/EA3/master/datos/J%26W/T4_6.DAT",
                sep = "", col.names = c("indep","supp","benev","conform","leader","sex","socio"))
dim(X)
head(X)
```

Hacemos las gráficas de los histogramas de cada una de las variables

```{r}
par(mfrow = c(2,3))
for(i in 1:5){ 
  hist(X[,i],breaks = 20, main = paste("Variable:",names(X)[i]), prob = T)
  a <- seq(min(X[,i]),max(X[,i]), length.out = nrow(X))
  lines(a,dnorm(a, mean = mean(X[,i]), sd = sd(X[,i])))
}

# Versión ggplot:
library(tidyverse)
X2 <- X %>% 
      mutate(sex = factor(sex),
             socio = factor(socio)) %>% 
      group_by(sex) %>% 
      pivot_longer(cols = indep:leader, 
                   names_to = "vars",
                   values_to = "valores") %>% 
            group_by(vars)
X2 %>% ggplot(aes(x = valores, fill = sex)) +
       geom_density() +
       facet_wrap(~ vars)
```

Se puede ver que los histogramas están un poco sesgados. Podemos probar formalmente normalidad, o hacer qq-plots

```{r}
for(i in 1:5){
  print(names(X)[i])
  print(shapiro.test(X[,i]))
}

# o también
Pruebas <- sapply(1:5,function(i)shapiro.test(X[,i]))
colnames(Pruebas) <- colnames(X)[1:5]
Pruebas[1:2,]
```

indep, supp y leader no pasan la prueba de normalidad. En estos casos, conviene buscar una transformación que normalice los datos.

```{r}
library(MASS)
b <- boxcox(lm(X$indep ~ 1)) # por ejemplo, raíz cuadrada

indep <- sqrt(X$indep)
hist(indep, prob =T)
a <- seq(min(indep),max(indep), length.out = length(indep))
lines(a,dnorm(a, mean = mean(indep), sd = sd(indep)))
shapiro.test(indep)  # mejora considerablemente 
# Para extraer el valor de lambda óptimo
(lam <-b$x[which.max(b$y)])

# BoxCox simultáneo:
library(car)
powerTransform(X[,c(1,2,5)])
powerTransform(X$indep)
```

Podemos hacer algunas pruebas de normalidad sobre algunas combinaciones lineales de las variables. Primero transformamos las variables:

```{r}
Y <- X[,1:5]  # hacemos una copia para hacer las transformaciones
Y$indep <- sqrt(X$indep)
Y$supp  <- X$supp^1.33
Y$leader <- X$leader^0.5
```

Ahora podemos hacer pruebas sobre combinaciones lineales aleatorias. Definimos un procedimiento que genere direcciones aleatorias de las variables que nos interesan

```{r}
par(mfrow=c(3,3))
for(i in 1:9){
  # generamos direcciones aleatorias en el disco unitario
  u <- mvrnorm(1, mu = rep(0,5), Sigma = diag(5))
  u <- u/sqrt(sum(u^2))
  # genera va's combinación lineales de las variables para cada dato de la muestra
  z <- apply(Y,1,function(x)sum(x*u))
  hist(z)

  # verifica normalidad
  print(shapiro.test(z))
}
```
